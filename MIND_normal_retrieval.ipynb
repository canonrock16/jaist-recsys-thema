{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79faac6d-e9a2-4f69-aaf5-9267f71dff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.model.retrieval_model import RetrievalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76da5a5e-32b6-4c8d-b643-5759670bb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rate = 0.2\n",
    "test_rate = 0.1\n",
    "batch_size = 100\n",
    "embedding_dimension = 100\n",
    "learning_rate = 0.1\n",
    "early_stopping_flg = True\n",
    "tensorboard_flg = False\n",
    "max_epoch_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f24e485-c2e4-4c02-b452-a13fbc2a51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_df = pd.read_table(\n",
    "    \"data/MIND/MINDsmall_train/behaviors.tsv\", names=(\"Impression_ID\", \"User_ID\", \"Time\", \"History\", \"Impressions\")\n",
    ")\n",
    "# news_df = pd.read_table(\n",
    "#     \"data/MIND/MINDsmall_train/news.tsv\",\n",
    "#     names=(\"News_ID\", \"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"Title_Entities\", \"Abstract_Entities\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db66be41-a177-4879-80c1-c1bd1f0b0e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Impression_ID</th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>History</th>\n",
       "      <th>Impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U13740</td>\n",
       "      <td>11/11/2019 9:05:58 AM</td>\n",
       "      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n",
       "      <td>N55689-1 N35729-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U91836</td>\n",
       "      <td>11/12/2019 6:11:30 PM</td>\n",
       "      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U73700</td>\n",
       "      <td>11/14/2019 7:01:48 AM</td>\n",
       "      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n",
       "      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U34670</td>\n",
       "      <td>11/11/2019 5:28:05 AM</td>\n",
       "      <td>N45729 N2203 N871 N53880 N41375 N43142 N33013 ...</td>\n",
       "      <td>N35729-0 N33632-0 N49685-1 N27581-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U8125</td>\n",
       "      <td>11/12/2019 4:11:21 PM</td>\n",
       "      <td>N10078 N56514 N14904 N33740</td>\n",
       "      <td>N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156960</th>\n",
       "      <td>156961</td>\n",
       "      <td>U21593</td>\n",
       "      <td>11/14/2019 10:24:05 PM</td>\n",
       "      <td>N7432 N58559 N1954 N43353 N14343 N13008 N28833...</td>\n",
       "      <td>N2235-0 N22975-0 N64037-0 N47652-0 N11378-0 N4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156961</th>\n",
       "      <td>156962</td>\n",
       "      <td>U10123</td>\n",
       "      <td>11/13/2019 6:57:04 AM</td>\n",
       "      <td>N9803 N104 N24462 N57318 N55743 N40526 N31726 ...</td>\n",
       "      <td>N3841-0 N61571-0 N58813-0 N28213-0 N4428-0 N25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156962</th>\n",
       "      <td>156963</td>\n",
       "      <td>U75630</td>\n",
       "      <td>11/14/2019 10:58:13 AM</td>\n",
       "      <td>N29898 N59704 N4408 N9803 N53644 N26103 N812 N...</td>\n",
       "      <td>N55913-0 N62318-0 N53515-0 N10960-0 N9135-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156963</th>\n",
       "      <td>156964</td>\n",
       "      <td>U44625</td>\n",
       "      <td>11/13/2019 2:57:02 PM</td>\n",
       "      <td>N4118 N47297 N3164 N43295 N6056 N38747 N42973 ...</td>\n",
       "      <td>N6219-0 N3663-0 N31147-0 N58363-0 N4107-0 N457...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>156965</td>\n",
       "      <td>U64800</td>\n",
       "      <td>11/14/2019 3:25:49 PM</td>\n",
       "      <td>N22997 N48742</td>\n",
       "      <td>N61233-0 N33828-1 N19661-0 N41934-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156965 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Impression_ID User_ID                    Time  \\\n",
       "0                   1  U13740   11/11/2019 9:05:58 AM   \n",
       "1                   2  U91836   11/12/2019 6:11:30 PM   \n",
       "2                   3  U73700   11/14/2019 7:01:48 AM   \n",
       "3                   4  U34670   11/11/2019 5:28:05 AM   \n",
       "4                   5   U8125   11/12/2019 4:11:21 PM   \n",
       "...               ...     ...                     ...   \n",
       "156960         156961  U21593  11/14/2019 10:24:05 PM   \n",
       "156961         156962  U10123   11/13/2019 6:57:04 AM   \n",
       "156962         156963  U75630  11/14/2019 10:58:13 AM   \n",
       "156963         156964  U44625   11/13/2019 2:57:02 PM   \n",
       "156964         156965  U64800   11/14/2019 3:25:49 PM   \n",
       "\n",
       "                                                  History  \\\n",
       "0       N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
       "1       N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
       "2       N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
       "3       N45729 N2203 N871 N53880 N41375 N43142 N33013 ...   \n",
       "4                             N10078 N56514 N14904 N33740   \n",
       "...                                                   ...   \n",
       "156960  N7432 N58559 N1954 N43353 N14343 N13008 N28833...   \n",
       "156961  N9803 N104 N24462 N57318 N55743 N40526 N31726 ...   \n",
       "156962  N29898 N59704 N4408 N9803 N53644 N26103 N812 N...   \n",
       "156963  N4118 N47297 N3164 N43295 N6056 N38747 N42973 ...   \n",
       "156964                                      N22997 N48742   \n",
       "\n",
       "                                              Impressions  \n",
       "0                                       N55689-1 N35729-0  \n",
       "1       N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n",
       "2       N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  \n",
       "3                     N35729-0 N33632-0 N49685-1 N27581-0  \n",
       "4       N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...  \n",
       "...                                                   ...  \n",
       "156960  N2235-0 N22975-0 N64037-0 N47652-0 N11378-0 N4...  \n",
       "156961  N3841-0 N61571-0 N58813-0 N28213-0 N4428-0 N25...  \n",
       "156962  N55913-0 N62318-0 N53515-0 N10960-0 N9135-0 N5...  \n",
       "156963  N6219-0 N3663-0 N31147-0 N58363-0 N4107-0 N457...  \n",
       "156964                N61233-0 N33828-1 N19661-0 N41934-0  \n",
       "\n",
       "[156965 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19e468e-3ebf-40c1-a2a4-d1d46122428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f592c06-a5f2-48a8-82b3-c83878cc2617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U32146    62\n",
       "U15740    44\n",
       "U20833    41\n",
       "U51286    40\n",
       "U44201    40\n",
       "          ..\n",
       "U60416     1\n",
       "U20588     1\n",
       "U84385     1\n",
       "U89164     1\n",
       "U72015     1\n",
       "Name: User_ID, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_df[\"User_ID\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8993f8cb-2e2c-4f0d-9199-83be2c16cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156965it [00:07, 20434.33it/s]\n"
     ]
    }
   ],
   "source": [
    "user2clicks = {}\n",
    "# for index,data in behaviors_df[:1000].iterrows():\n",
    "for index, data in tqdm(behaviors_df.iterrows()):\n",
    "    user = data[\"User_ID\"]\n",
    "    impressions = data[\"Impressions\"].split(\" \")\n",
    "    clicks = []\n",
    "    for impression in impressions:\n",
    "        # print(impression)\n",
    "        if impression[-1] == \"1\":\n",
    "            clicks.append(impression[:-2])\n",
    "    if user not in user2clicks:\n",
    "        user2clicks[user] = clicks\n",
    "    else:\n",
    "        user2clicks[user] = user2clicks[user] + clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e33906-f809-40c4-9faf-e5a25d0eff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 393846.89it/s]\n"
     ]
    }
   ],
   "source": [
    "user_list = []\n",
    "click_list = []\n",
    "for user, v in tqdm(user2clicks.items()):\n",
    "    for click in v:\n",
    "        user_list.append(user)\n",
    "        click_list.append(click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3f1130-3497-43c9-a0a0-09cb88fbc639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236344\n",
      "236344\n"
     ]
    }
   ],
   "source": [
    "print(len(user_list))\n",
    "print(len(click_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47cb3ea4-5f6f-4268-9777-7bd7f58784c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_df = pd.DataFrame(list(zip(user_list, click_list)), columns=[\"user_id\", \"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae0099ae-fc2e-4777-a0c2-48ca3a0db3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N55689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N28910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N58133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U91836</td>\n",
       "      <td>N17059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U91836</td>\n",
       "      <td>N26365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236339</th>\n",
       "      <td>U43157</td>\n",
       "      <td>N64152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236340</th>\n",
       "      <td>U43157</td>\n",
       "      <td>N41533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236341</th>\n",
       "      <td>U66493</td>\n",
       "      <td>N51048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236342</th>\n",
       "      <td>U66493</td>\n",
       "      <td>N11817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236343</th>\n",
       "      <td>U72015</td>\n",
       "      <td>N19318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236344 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id item_id\n",
       "0       U13740  N55689\n",
       "1       U13740  N28910\n",
       "2       U13740  N58133\n",
       "3       U91836  N17059\n",
       "4       U91836  N26365\n",
       "...        ...     ...\n",
       "236339  U43157  N64152\n",
       "236340  U43157  N41533\n",
       "236341  U66493  N51048\n",
       "236342  U66493  N11817\n",
       "236343  U72015  N19318\n",
       "\n",
       "[236344 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed7d86fe-ccba-48c6-9fe0-838c2e506293",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices({\"user_id\": click_df[\"user_id\"], \"item_id\": click_df[\"item_id\"]})\n",
    "# train_size = int(len(click_df) * (1 - test_rate))\n",
    "# val_size = int(train_size * (1 - val_rate))\n",
    "val_size = int(len(click_df) * val_rate)\n",
    "test_size = int(len(click_df) * test_rate)\n",
    "train_size = len(click_df) - val_size - test_size\n",
    "train = ratings.take(train_size).batch(batch_size)\n",
    "val = ratings.skip(train_size).take(val_size).batch(batch_size)\n",
    "test = ratings.skip(train_size + val_size).take(test_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0f550b7-4e1c-4edf-8b1c-17383e7d64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = np.array(list(set(user_list)))\n",
    "unique_item_ids = np.array(list(set(click_list)))\n",
    "unique_item_dataset = tf.data.Dataset.from_tensor_slices(unique_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4708c871-141f-42a2-817f-a9be3d3c8308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7713"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_user_ids.size\n",
    "unique_item_ids.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a94a34a-0148-46fe-83b9-1a6fc01bad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haruka/dev/jaist-recsys-thema/.venv/lib/python3.9/site-packages/numpy/core/numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = RetrievalModel(\n",
    "        unique_user_ids=unique_user_ids,\n",
    "        unique_item_ids=unique_item_ids,\n",
    "        user_dict_key=\"user_id\",\n",
    "        item_dict_key=\"item_id\",\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        metrics_candidate_dataset=unique_item_dataset,\n",
    "    )\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c2e1f05-f0ba-4f71-ad14-af508d275827",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "if early_stopping_flg:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"total_loss\",\n",
    "            min_delta=0,\n",
    "            patience=3,\n",
    "            verbose=0,\n",
    "            mode=\"auto\",\n",
    "            baseline=None,\n",
    "            restore_best_weights=False,\n",
    "        )\n",
    "    )\n",
    "if tensorboard_flg:\n",
    "    tfb_log_path = log_path + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tfb_log_path,\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56d9fb14-afba-463d-b2ff-5bd3d602d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 15:41:34.965486: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 236344\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:4355\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-09-10 15:41:35.026123: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654/1655 [============================>.] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 538.7877 - regularization_loss: 0.0000e+00 - total_loss: 538.7877"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 15:42:34.311239: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 236344\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:4355\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-09-10 15:42:34.355268: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655/1655 [==============================] - 89s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 538.3450 - regularization_loss: 0.0000e+00 - total_loss: 538.3450 - val_factorized_top_k/top_1_categorical_accuracy: 1.2694e-04 - val_factorized_top_k/top_5_categorical_accuracy: 5.5006e-04 - val_factorized_top_k/top_10_categorical_accuracy: 0.0011 - val_factorized_top_k/top_50_categorical_accuracy: 0.0055 - val_factorized_top_k/top_100_categorical_accuracy: 0.0113 - val_loss: 312.0023 - val_regularization_loss: 0.0000e+00 - val_total_loss: 312.0023\n",
      "Epoch 2/20\n",
      "1655/1655 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 19484.0581 - regularization_loss: 0.0000e+00 - total_loss: 19484.0581"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 15:44:02.502180: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655/1655 [==============================] - 90s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 19479.0776 - regularization_loss: 0.0000e+00 - total_loss: 19479.0776 - val_factorized_top_k/top_1_categorical_accuracy: 8.4624e-05 - val_factorized_top_k/top_5_categorical_accuracy: 4.6543e-04 - val_factorized_top_k/top_10_categorical_accuracy: 9.7317e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0070 - val_factorized_top_k/top_100_categorical_accuracy: 0.0146 - val_loss: 370.3233 - val_regularization_loss: 0.0000e+00 - val_total_loss: 370.3233\n",
      "Epoch 3/20\n",
      "   5/1655 [..............................] - ETA: 55s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 32328.3785 - regularization_loss: 0.0000e+00 - total_loss: 32328.3785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 15:44:34.341094: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654/1655 [============================>.] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 36889.1703 - regularization_loss: 0.0000e+00 - total_loss: 36889.1703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 15:45:34.116523: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655/1655 [==============================] - 89s 54ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 36863.3757 - regularization_loss: 0.0000e+00 - total_loss: 36863.3757 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 1.4809e-04 - val_factorized_top_k/top_10_categorical_accuracy: 5.5006e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0034 - val_factorized_top_k/top_100_categorical_accuracy: 0.0077 - val_loss: 369.5983 - val_regularization_loss: 0.0000e+00 - val_total_loss: 369.5983\n",
      "Epoch 4/20\n",
      "1654/1655 [============================>.] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45057.1254 - regularization_loss: 0.0000e+00 - total_loss: 45057.1254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 15:47:01.822633: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655/1655 [==============================] - 88s 53ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 45022.3190 - regularization_loss: 0.0000e+00 - total_loss: 45022.3190 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 2.1156e-05 - val_factorized_top_k/top_10_categorical_accuracy: 2.9618e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0030 - val_factorized_top_k/top_100_categorical_accuracy: 0.0077 - val_loss: 355.3127 - val_regularization_loss: 0.0000e+00 - val_total_loss: 355.3127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1370d97f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train, validation_data=val, epochs=max_epoch_num, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51a80147-c6b3-4b09-b9a7-63dbbcbd5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 15:48:18.714571: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 236344\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:4355\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-09-10 15:48:18.767661: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - 17s 71ms/step - factorized_top_k/top_1_categorical_accuracy: 4.2312e-05 - factorized_top_k/top_5_categorical_accuracy: 2.1156e-04 - factorized_top_k/top_10_categorical_accuracy: 3.8081e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0025 - factorized_top_k/top_100_categorical_accuracy: 0.0066 - loss: 537.3360 - regularization_loss: 0.0000e+00 - total_loss: 537.3360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 4.231192360748537e-05,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.00021155961439944804,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.00038080732338130474,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.002496403409168124,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.0066006602719426155,\n",
       " 'loss': 136.3968963623047,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 136.3968963623047}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b218f4-4455-4dd1-a992-893d4ea9b364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
