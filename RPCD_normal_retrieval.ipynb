{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79faac6d-e9a2-4f69-aaf5-9267f71dff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.model.retrieval_model import RetrievalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76da5a5e-32b6-4c8d-b643-5759670bb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rate = 0.2\n",
    "test_rate = 0.1\n",
    "batch_size = 100\n",
    "embedding_dimension = 50\n",
    "learning_rate = 0.1\n",
    "early_stopping_flg = True\n",
    "tensorboard_flg = False\n",
    "max_epoch_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f24e485-c2e4-4c02-b452-a13fbc2a51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_df = pd.read_csv(\n",
    "    \"data/RentalProperties/user_activity.csv\", names=(\"item_id\", \"user_id\", \"event_type\", \"create_timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b4b658-17dd-4a0c-9906-f00cda4079f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# behaviors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f592c06-a5f2-48a8-82b3-c83878cc2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_df = behaviors_df.query('event_type == \"seen\"')\n",
    "count_df = pd.DataFrame(seen_df[\"user_id\"].value_counts()).reset_index().rename(columns={\"index\": \"user_id\", \"user_id\": \"count\"})\n",
    "\n",
    "unique_user_ids = list(count_df.query(\"count >= 10\")[\"user_id\"])\n",
    "seen_df = seen_df[seen_df[\"user_id\"].isin(unique_user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f597aa-e517-4e6a-b27c-c5a467f223a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_in_list_df = (\n",
    "    behaviors_df.query('event_type == \"seen_in_list\"')\n",
    "    .groupby([\"user_id\", \"item_id\"])\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index(name=\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb77aefc-f424-4dce-b9eb-4577ee8f0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(seen_df, test_size=0.1, stratify=seen_df[\"user_id\"], random_state=1)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df[\"user_id\"], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e452e9f7-3c2d-43ba-a6e8-b36007f8b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズで割り切れるように丸める\n",
    "step_size = int(len(train_df) / batch_size)\n",
    "train_df = train_df[: step_size * batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0027e9f9-5847-4dd0-b2c2-b2a30d40f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6562362-c613-453b-a95e-94d3fe766983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2435\n",
      "2435\n",
      "2435\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df[\"user_id\"].unique()))\n",
    "print(len(val_df[\"user_id\"].unique()))\n",
    "print(len(test_df[\"user_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02af68ac-dbcd-4a2f-9019-ae24296498b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 09:26:34.801665: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ratings = tf.data.Dataset.from_tensor_slices({\"user_id\": train_df[\"user_id\"], \"item_id\": train_df[\"item_id\"]})\n",
    "val_ratings = tf.data.Dataset.from_tensor_slices({\"user_id\": val_df[\"user_id\"], \"item_id\": val_df[\"item_id\"]})\n",
    "test_ratings = tf.data.Dataset.from_tensor_slices({\"user_id\": test_df[\"user_id\"], \"item_id\": test_df[\"item_id\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f7fedd-7c3b-4a88-aef8-73b749e94e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_ratings.batch(batch_size)\n",
    "val = val_ratings.batch(batch_size)\n",
    "test = test_ratings.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "243eab2a-85b3-4165-86ee-3da5567411e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = np.array(\n",
    "    list((set(train_df[\"user_id\"].unique()) | set(val_df[\"user_id\"].unique()) | set(test_df[\"user_id\"].unique())))\n",
    ")\n",
    "unique_item_ids = np.array(\n",
    "    list(set(train_df[\"item_id\"].unique()) | set(val_df[\"item_id\"].unique()) | set(test_df[\"item_id\"].unique()))\n",
    ")\n",
    "unique_item_dataset = tf.data.Dataset.from_tensor_slices(unique_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fbe0236-fd26-4bc0-8907-9ba8966e90ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3893/3893 [00:35<00:00, 108.16it/s]\n"
     ]
    }
   ],
   "source": [
    "user_id2seen_items = {}\n",
    "seen_user_ids = list(seen_in_list_df[\"user_id\"].unique())\n",
    "for seen_user_id in tqdm(seen_user_ids):\n",
    "    user_id2seen_items[seen_user_id] = []\n",
    "    seen_items = seen_in_list_df.query(f'user_id == \"{seen_user_id}\"')\n",
    "    for i, item in seen_items.iterrows():\n",
    "        user_id2seen_items[seen_user_id].append({\"item_id\": item[\"item_id\"], \"count\": item[\"count\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b355d7b0-793a-417e-b0b8-6a449977b8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 406/406 [00:52<00:00,  7.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "item_weights = []\n",
    "# for batch in train.take(10):\n",
    "# for batch in tqdm(train.take(100)):\n",
    "for batch in tqdm(train):\n",
    "    # そのバッチに含まれるuser_idとitem_id達\n",
    "    user_ids = batch[\"user_id\"].numpy()\n",
    "    item_ids = batch[\"item_id\"].numpy()\n",
    "\n",
    "    item_weights_by_batch = []\n",
    "    for i, user_id in enumerate(user_ids):\n",
    "        # 基本のweightsは1にする\n",
    "        weights = np.ones(len(item_ids), dtype=\"float32\")\n",
    "\n",
    "        decoded_user_id = user_id.decode(\"utf-8\")\n",
    "        # もしそのユーザーのviewログがあるアイテムがあり、かつそのアイテムがバッチの中に存在して、ユーザーがクリックしていなかったら、weightを上げる\n",
    "        if decoded_user_id in user_id2seen_items:\n",
    "            seen_items = user_id2seen_items[decoded_user_id]\n",
    "            # 各seen_itemがバッチの中に存在するか？\n",
    "            for seen_item in seen_items:\n",
    "                for j, item_id in enumerate(item_ids):\n",
    "                    decoded_item_id = item_id.decode(\"utf-8\")\n",
    "                    if seen_item[\"item_id\"] == decoded_item_id and i != j:\n",
    "                        # weights[j] = seen_item[\"count\"] + 1\n",
    "                        weights[j] = seen_item[\"count\"]\n",
    "\n",
    "        item_weights_by_batch.append(weights)\n",
    "    item_weights.append(item_weights_by_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f43b8e0-6b89-4da0-8ec5-25fd779ecd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 100, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_weights = np.array(item_weights)\n",
    "item_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddbc135-616a-4451-a111-cf582bf29212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_weights.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32c3b68-46ec-4b1f-8c85-29ab2903b938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_weights.reshape([step_size * batch_size, batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e55e6fa-f348-4685-a877-e2a176d6ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings = tf.data.Dataset.from_tensor_slices(\n",
    "    {\n",
    "        \"user_id\": train_df[\"user_id\"],\n",
    "        \"item_id\": train_df[\"item_id\"],\n",
    "        \"item_weights\": item_weights.reshape([step_size * batch_size, batch_size]),\n",
    "    }\n",
    ")\n",
    "train2 = train_ratings.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c9f4278-f08c-4492-a4aa-e52ad12be10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexes = np.where(item_weights == 2)\n",
    "# item_weightが2になっているインデックスに相当するユーザーとアイテムが、本当にseen_in_list_dfにあるかどうか検査\n",
    "for i, j, k in zip(indexes[0], indexes[1], indexes[2]):\n",
    "    if i == 0:\n",
    "        for batch in train2.take(1):\n",
    "            user_ids = batch[\"user_id\"].numpy()\n",
    "            item_ids = batch[\"item_id\"].numpy()\n",
    "            user_id = user_ids[j].decode(\"utf-8\")\n",
    "            item_id = item_ids[k].decode(\"utf-8\")\n",
    "\n",
    "            result = seen_in_list_df.query(f'user_id == \"{user_id}\" and item_id == \"{item_id}\"')\n",
    "            if len(result) == 0:\n",
    "                print(\"zero\")\n",
    "            else:\n",
    "                # print('ok')\n",
    "                pass\n",
    "# item_weightの対角成分が2になっていないことを確認\n",
    "for i, j, k in zip(indexes[0], indexes[1], indexes[2]):\n",
    "    if j == k:\n",
    "        print(\"NG\")\n",
    "        \n",
    "# item_weightsの内容と、trainから出てくる内容が同一であることをチェック\n",
    "for i, batch in enumerate(train2):\n",
    "    if (item_weights[i] != batch[\"item_weights\"]).numpy().all():\n",
    "        print(\"NG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a94a34a-0148-46fe-83b9-1a6fc01bad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haruka/dev/jaist-recsys-thema/.venv/lib/python3.9/site-packages/numpy/core/numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = RetrievalModel(\n",
    "        unique_user_ids=unique_user_ids,\n",
    "        unique_item_ids=unique_item_ids,\n",
    "        user_dict_key=\"user_id\",\n",
    "        item_dict_key=\"item_id\",\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        metrics_candidate_dataset=unique_item_dataset,\n",
    "        # num_hard_negatives=0,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM),\n",
    "        # loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM),\n",
    "        # loss=Customloss(),\n",
    "        # loss=custom_loss_function,\n",
    "    )\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c2e1f05-f0ba-4f71-ad14-af508d275827",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "if early_stopping_flg:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_total_loss\",\n",
    "            min_delta=0,\n",
    "            patience=3,\n",
    "            verbose=0,\n",
    "            mode=\"auto\",\n",
    "            baseline=None,\n",
    "            restore_best_weights=False,\n",
    "        )\n",
    "    )\n",
    "if tensorboard_flg:\n",
    "    tfb_log_path = log_path + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tfb_log_path,\n",
    "            histogram_freq=1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56d9fb14-afba-463d-b2ff-5bd3d602d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 09:28:04.373262: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_3\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_FLOAT\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 40600\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:8\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 100\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-10-10 09:28:04.418828: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "397/406 [============================>.] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 22.8042 - regularization_loss: 0.0000e+00 - total_loss: 22.8042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 09:28:07.251842: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10168\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:1\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406/406 [==============================] - 9s 20ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 22.5775 - regularization_loss: 0.0000e+00 - total_loss: 22.5775 - val_factorized_top_k/top_1_categorical_accuracy: 4.9174e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0016 - val_factorized_top_k/top_10_categorical_accuracy: 0.0026 - val_factorized_top_k/top_50_categorical_accuracy: 0.0070 - val_factorized_top_k/top_100_categorical_accuracy: 0.0099 - val_loss: 15.4017 - val_regularization_loss: 0.0000e+00 - val_total_loss: 15.4017\n",
      "Epoch 2/20\n",
      "406/406 [==============================] - 7s 18ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13.8162 - regularization_loss: 0.0000e+00 - total_loss: 13.8162 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0020 - val_factorized_top_k/top_10_categorical_accuracy: 0.0046 - val_factorized_top_k/top_50_categorical_accuracy: 0.0101 - val_factorized_top_k/top_100_categorical_accuracy: 0.0155 - val_loss: 16.1064 - val_regularization_loss: 0.0000e+00 - val_total_loss: 16.1064\n",
      "Epoch 3/20\n",
      "406/406 [==============================] - 8s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 15.5113 - regularization_loss: 0.0000e+00 - total_loss: 15.5113 - val_factorized_top_k/top_1_categorical_accuracy: 9.8348e-05 - val_factorized_top_k/top_5_categorical_accuracy: 0.0026 - val_factorized_top_k/top_10_categorical_accuracy: 0.0060 - val_factorized_top_k/top_50_categorical_accuracy: 0.0173 - val_factorized_top_k/top_100_categorical_accuracy: 0.0231 - val_loss: 17.4706 - val_regularization_loss: 0.0000e+00 - val_total_loss: 17.4706\n",
      "Epoch 4/20\n",
      "406/406 [==============================] - 7s 18ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13.5386 - regularization_loss: 0.0000e+00 - total_loss: 13.5386 - val_factorized_top_k/top_1_categorical_accuracy: 3.9339e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0028 - val_factorized_top_k/top_10_categorical_accuracy: 0.0075 - val_factorized_top_k/top_50_categorical_accuracy: 0.0209 - val_factorized_top_k/top_100_categorical_accuracy: 0.0275 - val_loss: 13.9074 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13.9074\n",
      "Epoch 5/20\n",
      " 41/406 [==>...........................] - ETA: 1s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13.6740 - regularization_loss: 0.0000e+00 - total_loss: 13.6740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 09:28:36.659942: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406/406 [==============================] - 8s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 12.2547 - regularization_loss: 0.0000e+00 - total_loss: 12.2547 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0032 - val_factorized_top_k/top_10_categorical_accuracy: 0.0065 - val_factorized_top_k/top_50_categorical_accuracy: 0.0222 - val_factorized_top_k/top_100_categorical_accuracy: 0.0295 - val_loss: 14.7961 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14.7961\n",
      "Epoch 6/20\n",
      "406/406 [==============================] - 8s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 12.7346 - regularization_loss: 0.0000e+00 - total_loss: 12.7346 - val_factorized_top_k/top_1_categorical_accuracy: 9.8348e-05 - val_factorized_top_k/top_5_categorical_accuracy: 0.0012 - val_factorized_top_k/top_10_categorical_accuracy: 0.0047 - val_factorized_top_k/top_50_categorical_accuracy: 0.0190 - val_factorized_top_k/top_100_categorical_accuracy: 0.0290 - val_loss: 16.3392 - val_regularization_loss: 0.0000e+00 - val_total_loss: 16.3392\n",
      "Epoch 7/20\n",
      "406/406 [==============================] - 9s 23ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 14.7225 - regularization_loss: 0.0000e+00 - total_loss: 14.7225 - val_factorized_top_k/top_1_categorical_accuracy: 9.8348e-05 - val_factorized_top_k/top_5_categorical_accuracy: 0.0020 - val_factorized_top_k/top_10_categorical_accuracy: 0.0053 - val_factorized_top_k/top_50_categorical_accuracy: 0.0223 - val_factorized_top_k/top_100_categorical_accuracy: 0.0289 - val_loss: 19.6173 - val_regularization_loss: 0.0000e+00 - val_total_loss: 19.6173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x169787400>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x=train, validation_data=val, epochs=max_epoch_num, callbacks=callbacks)\n",
    "model.fit(x=train2, validation_data=val, epochs=max_epoch_num, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51a80147-c6b3-4b09-b9a7-63dbbcbd5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/57 [..............................] - ETA: 10s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0100 - factorized_top_k/top_10_categorical_accuracy: 0.0200 - factorized_top_k/top_50_categorical_accuracy: 0.0300 - factorized_top_k/top_100_categorical_accuracy: 0.0600 - loss: 20.4209 - regularization_loss: 0.0000e+00 - total_loss: 20.4209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 09:29:01.446957: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 5649\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:2\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 73ms/step - factorized_top_k/top_1_categorical_accuracy: 1.7702e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0016 - factorized_top_k/top_10_categorical_accuracy: 0.0042 - factorized_top_k/top_50_categorical_accuracy: 0.0198 - factorized_top_k/top_100_categorical_accuracy: 0.0281 - loss: 20.0856 - regularization_loss: 0.0000e+00 - total_loss: 20.0856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0001770224771462381,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0015932023525238037,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.004248539451509714,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.01982651837170124,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.028146574273705482,\n",
       " 'loss': 19.177644729614258,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 19.177644729614258}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb694f-6320-42b8-b02a-f26d14c8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb015210-34c6-46bd-98ea-1be0c644bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# behaviors_df.query('event_type == \"seen\" or event_type == \"seen_in_list\"').groupby([\"user_id\", \"item_id\"]).size().sort_values(\n",
    "#     ascending=False\n",
    "# ).reset_index(name=\"count\").query(\"count > 1\")\n",
    "# seen_in_list_items = set(behaviors_df.query('event_type == \"seen_in_list\"')[\"item_id\"].unique())\n",
    "# seen_items = set(behaviors_df.query('event_type == \"seen\"')[\"item_id\"].unique())\n",
    "# どのユーザーがどのアイテムをseen_in_listしてたか。ここにある組み合わせが0として入ってきたらlossを増やしてあげたい。\n",
    "# behaviors_df.query('event_type == \"seen_in_list\"')\n",
    "\n",
    "# behaviors_df.query('event_type == \"seen\"')[\"user_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6eb6f9-60f0-4603-b42d-ca423f52904f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9ac38-f9c7-47b4-ab64-60e04feac5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a6aa515-2aa3-402a-a7a2-7b5d027163b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weightsを事前に作っておく\n",
    "# sample_weights = []\n",
    "# # for batch in train.take(10):\n",
    "# # for batch in tqdm(train.take(100)):\n",
    "# for batch in tqdm(train):\n",
    "#     # そのバッチに含まれるuser_idとitem_id達\n",
    "#     user_ids = batch[\"user_id\"].numpy()\n",
    "#     item_ids = batch[\"item_id\"].numpy()\n",
    "\n",
    "#     sample_weights_by_batch = []\n",
    "#     for i, user_id in enumerate(user_ids):\n",
    "#         # 基本のweightsは1にする\n",
    "#         weights = np.ones(len(item_ids))\n",
    "#         # weights = 1\n",
    "\n",
    "#         decoded_user_id = user_id.decode(\"utf-8\")\n",
    "#         # もしそのユーザーのviewログがあるアイテムがあり、かつそのアイテムがバッチの中に存在して、ユーザーがクリックしていなかったら、weightを上げる\n",
    "#         if decoded_user_id in user_id2seen_items:\n",
    "#             seen_items = user_id2seen_items[decoded_user_id]\n",
    "#             # 各seen_itemがバッチの中に存在するか？\n",
    "#             for seen_item in seen_items:\n",
    "#                 for j, item_id in enumerate(item_ids):\n",
    "#                     decoded_item_id = item_id.decode(\"utf-8\")\n",
    "#                     if seen_item[\"item_id\"] == decoded_item_id and i != j:\n",
    "#                         weights[j] = seen_item[\"count\"] + 1\n",
    "\n",
    "#         sample_weights_by_batch.append(weights)\n",
    "#         # sample_weights.append(weights)\n",
    "#     sample_weights.append(sample_weights_by_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b99632-bd4a-4e5e-b6a0-7481cf168bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0febd2c1-5764-42d0-84da-9624cdfa221e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec293ba2-cf3a-42c7-84d7-bce4b85f8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([[7, 7, 7, 7], [7, 7, 7, 7], [7, 7, 7, 7], [7, 7, 7, 7]])  # Input shape: (2, 3, 4)\n",
    "diagonal = np.array([1, 1, 1, 1])  # Diagonal shape: (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "903c8c72-5de9-4799-be9b-b7592133a536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\n",
       "array([[1, 7, 7, 7],\n",
       "       [7, 1, 7, 7],\n",
       "       [7, 7, 1, 7],\n",
       "       [7, 7, 7, 1]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.set_diag(input, diagonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db9678de-cdd2-4efe-945e-81a7a33621c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4fbf48-8f40-4852-9bfc-f7367ff0d9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d92131d0-f127-491e-afda-64c2f757433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = []\n",
    "for i in range(3):\n",
    "    aaa.append(np.identity(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb7fa0e5-a256-4eea-a2ff-8858959cffcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array(aaa)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62e745f7-52ac-455c-9cdd-e7bbabb0c173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08a8dd0b-785d-415d-8ece-4ac3cb2c19a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.reshape([15, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4fff567-4c83-4c7f-9e67-002b437480c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        target = tf.convert_to_tensor(y_true)\n",
    "        output = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "        output, from_logits = _get_logits(output, from_logits, \"Sigmoid\", \"binary_crossentropy\")\n",
    "        # if from_logits:\n",
    "        # return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n",
    "\n",
    "        epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "        output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n",
    "\n",
    "        # Compute cross entropy from probabilities.\n",
    "        bce = target * tf.math.log(output + epsilon())\n",
    "        bce += (1 - target) * tf.math.log(1 - output + epsilon())\n",
    "        return -bce\n",
    "\n",
    "\n",
    "def custom_loss_function(y_true, y_pred, sample_weight=None):\n",
    "    # aaa = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "    # return aaa(y_true, y_pred)\n",
    "    loss = tf.reduce_mean(tf.math.abs((y_true - y_pred) ** 3))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90c490cd-0971-41d2-89c0-cf66dcf890d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankL(np_rank):\n",
    "    r = int(np_rank[-1])\n",
    "    _l = 0\n",
    "    for k in range(1, r + 1):\n",
    "        _l += 1.0 / k\n",
    "    return np.float32(_l)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "labels are assumed to be 1 hot encoded\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def warp_loss(labels, logits, sample_weights=None):\n",
    "    # for easy broadcasting\n",
    "    labels, logits = tf.transpose(labels, [1, 0]), tf.transpose(logits, [1, 0])\n",
    "    f_y = tf.reduce_sum(logits * labels, axis=0)\n",
    "    rank = tf.reduce_sum(tf.maximum(tf.sign(1 + logits - f_y), 0), axis=0)\n",
    "    diff = tf.reduce_sum(tf.maximum(1 + logits - f_y, 0), axis=0)\n",
    "    with tf.control_dependencies([tf.assert_greater(rank, tf.zeros_like(rank))]):\n",
    "        return tf.py_func(rankL, [rank], tf.float32) * diff / rank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
